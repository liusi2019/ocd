{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: histogram\n",
      "\n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "The following object is masked from ‘package:MASS’:\n",
      "\n",
      "    select\n",
      "\n",
      "The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(\"btLoda\")\n",
    "library(\"histogram\")\n",
    "library(\"MASS\")\n",
    "library(\"dplyr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.2\n"
     ]
    }
   ],
   "source": [
    "vpro = 20  # percentage of anomaly\n",
    "alpha = vpro/100  # anomaly proportion in the mixture data set\n",
    "\n",
    "print(alpha)\n",
    "\n",
    "runs=2 ## 100 for actual experiments\n",
    "comp_gt_perf=data.frame()\n",
    "comp_perf1_e02=data.frame();comp_perf1_e05=data.frame();comp_perf1_e10=data.frame();\n",
    "comp_perf1_loda_e02=data.frame();comp_perf1_loda_e05=data.frame();comp_perf1_loda_e10=data.frame();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### datab: anomlay scores for the second clean data set\n",
    "\n",
    "###### datan: anomaly scores for the first mixture data set\n",
    "\n",
    "###### alpha: anomaly proportion\n",
    "\n",
    "###### epsilon: which quantile of the anomaly score of anomlay points are we focusing on\n",
    "\n",
    "###### nboots: number of bootstraps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to get quantile estimates in our setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAW ECDF i.e. emperical cdf \n",
    "raw_cdf<- function(datab, datan, alpha, epsilon){\n",
    "  trialv <- sort(c(datan, datab))\n",
    "  F.n <- ecdf(datan)\n",
    "  Fn  <- F.n(trialv)\n",
    "  F.b <- ecdf(datab)\n",
    "  Fb <- F.b(trialv)\n",
    "  est=c()\n",
    "  for (j in 0:5){ #   ## To get statistcs for over estimates[alpha_e] of \\alpha    \n",
    "    alpha_e=(1+0.02*j)*alpha   \n",
    "  Fa <- (Fn - (1-alpha_e)*Fb)/alpha_e \n",
    "  if (length(which(Fa <= epsilon))==0){\n",
    "    index <- 1\n",
    "  }else{\n",
    "    index <-max(which(Fa <= epsilon))\n",
    "  }\n",
    "  est[j+1]=trialv[index]\n",
    "  }\n",
    "  return (est)\n",
    "}\n",
    "\n",
    "## Isotonic Regression emperical CDF so that the CDF is Monotonic.\n",
    "\n",
    "iso_cdf <- function(datab, datan, alpha, epsilon){\n",
    "  trialv <- sort(c(datan, datab))\n",
    "  F.n <- ecdf(datan)\n",
    "  Fn  <- F.n(trialv)\n",
    "  F.b <- ecdf(datab)\n",
    "  Fb <- F.b(trialv)\n",
    "\n",
    "  est=c()\n",
    "  for (j in 0:5){\n",
    "    alpha_e=(1+0.02*j)*alpha \n",
    "  Fa <- (Fn - (1-alpha_e)*Fb)/alpha_e \n",
    "  F.is = isoreg(Fa)$yf ## Computes the Isotonic Estimator of Fa\n",
    "  F.is[which(F.is<=0)]=0  \n",
    "  F.is[which(F.is>=1)]=1\n",
    "  if (length(which(F.is <= epsilon))==0){\n",
    "    index.is <- 1\n",
    "  }else{\n",
    "    index.is <-max(which(F.is <= epsilon))\n",
    "  }\n",
    "  est[j+1]=trialv[index.is]\n",
    "  }\n",
    "  return (est)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below function returns the Recall, False Positive Rate for both Raw cdf and Isotonized CDF for various values of alien proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to use this function, for the mixture data set, the anomaly points need to be put in the beginning of the data set. \n",
    "\n",
    "cv_result <- function(datab, datan, alpha, epsilon){\n",
    "  ## 10 fold cross validation\n",
    "  nn <- length(datan)\n",
    "  na <- round(nn*alpha) ## number of anomaly points. \n",
    "  if (nn/10 - floor(nn/10) > 0){\n",
    "    group <- c(rep((1:10), floor(nn/10)), (1:(nn - 10*floor(nn/10))))\n",
    "  }else{\n",
    "    group <- rep((1:10), floor(nn/10))\n",
    "  }\n",
    "  group.label <- sample(group)\n",
    "\n",
    "  ## build a data frame to help with grouping and identifying anomaly points\n",
    "  ## the first column is for group id\n",
    "  ## the second column is the row number of each point. All points with row number smaller or equal than na are actual anomaly points\n",
    "  df <- data.frame(group.label = group.label, index = (1:nn))\n",
    "\n",
    "  ## for direct ECDF\n",
    "  correct.vec2 <- rep(0, 10) \n",
    "  wrong.vec2 <- rep(0, 10) \n",
    "  ano.vec2 <- rep(0, 10)\n",
    "  nom.vec2 <- rep(0, 10)\n",
    "  ## for isotonized ECDF\n",
    "  correct.vec3 <- rep(0, 10)\n",
    "  wrong.vec3 <- rep(0, 10) \n",
    "  ano.vec3 <- rep(0, 10)\n",
    "  nom.vec3 <- rep(0, 10)\n",
    "\n",
    "  # result <- list()\n",
    "  result=data.frame()\n",
    "  \n",
    "  est.vec2 <- rep(0, 10)\n",
    "  est.vec3 <- rep(0, 10)\n",
    " for (j in 0:5){\n",
    "  alpha_e=0.002*j+alpha \n",
    "    for (i in 1:10){# cross validation\n",
    "      use_datan <- datan[which(df$group.label!= i)] ## anomaly scores from the 9 folds to use\n",
    "      test_datan <- datan[which(df$group.label == i)] ## anomaly scores from the left out 1 fold to use as testing data\n",
    "      use_df <- df[which(df$group.label!= i),] ## the part of group label dataframe that correspond to the 9 folds\n",
    "      test_df <- df[which(df$group.label == i),] ## the part of group label dataframe that correspond to the 1 fold\n",
    "      index_use <- use_df$index ## index of the 9 folds\n",
    "      index_test <- test_df$index ## index of the 1 fold\n",
    "      est2 <- raw_cdf(datab, use_datan, alpha, epsilon) ## estimate from raw ECDF method\n",
    "      est3 <- iso_cdf(datab, use_datan, alpha, epsilon) ## estiamte from isotonized ECDF method\n",
    "         \n",
    "      if (length(which(test_datan >= est2[j+1]))==0){\n",
    "        correct.vec2[i] <- 0 ## number of correctly claimed anomaly points\n",
    "        wrong.vec2[i] <- 0\n",
    "      }else{\n",
    "        pos2 <- index_test[which(test_datan >= est2[j+1])] ## the row numbers of the points in the 1 fold test data which are claimed to be anomaly\n",
    "        correct.vec2[i] <- sum(pos2 <= na) \n",
    "        wrong.vec2[i] <- sum(pos2 > na) \n",
    "      }\n",
    "      ano.vec2[i] <- sum(index_test <= na) \n",
    "      nom.vec2[i] <- sum(index_test > na) \n",
    "    \n",
    "      if (length(which(test_datan >= est3[j+1]))==0){\n",
    "        correct.vec3[i] <- 0 ## number of correctly claimed anomaly points\n",
    "        wrong.vec3[i] <- 0\n",
    "      }else{\n",
    "        pos3 <- index_test[which(test_datan >= est3[j+1])] ## the row numbers of the points in the 1 fold test data which are claimed to be anomaly\n",
    "        correct.vec3[i] <- sum(pos3 <= na) \n",
    "        wrong.vec3[i] <- sum(pos3 > na)\n",
    "      }\n",
    "      ano.vec3[i] <- sum(index_test <=na)\n",
    "      nom.vec3[i] <- sum(index_test > na)  \n",
    "      \n",
    "      \n",
    "    \n",
    "     result1=rep(0,7)\n",
    "     \n",
    "     result1[1] <- sum(correct.vec2)/sum(ano.vec2) \n",
    "     result1[2] <- sum(wrong.vec2)/(sum(correct.vec2) + sum(wrong.vec2)) \n",
    "     result1[3] <- sum(wrong.vec2)/sum(nom.vec2) \n",
    "    \n",
    "     result1[4] <- sum(correct.vec3)/sum(ano.vec3) \n",
    "     result1[5] <- sum(wrong.vec3)/(sum(correct.vec3) + sum(wrong.vec3)) \n",
    "     result1[6] <- sum(wrong.vec3)/sum(nom.vec3)\n",
    "     \n",
    "     result1[7]=alpha_e\n",
    "     \n",
    "     result=rbind(result,result1)\n",
    "   }\n",
    "  }\n",
    "   return(result)\n",
    " }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below function returns average performance for all (100) runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "overall_perf <- function(comb_perf,epsilon){\n",
    "  raw_cdf_perf=comb_perf\n",
    "  temp_ovr_perf=data.frame()\n",
    "  \n",
    "  for (i in 1:as.integer(ncol(raw_cdf_perf)/7)){\n",
    "\n",
    "    temp_ovr_perf1=data.frame()  \n",
    "    \n",
    "    temp_ovr_perf1=c(\n",
    "            mean(raw_cdf_perf[[1+(7*(i-1))]],na.rm=TRUE),sd(raw_cdf_perf[[1+(7*(i-1))]],na.rm=TRUE),#recall raw cdf\n",
    "                    mean(raw_cdf_perf[[3+(7*(i-1))]],na.rm=TRUE),sd(raw_cdf_perf[[3+(7*(i-1))]],na.rm=TRUE),#FPR raw CDf\n",
    "                    mean(raw_cdf_perf[[4+(7*(i-1))]],na.rm=TRUE),sd(raw_cdf_perf[[4+(7*(i-1))]],na.rm=TRUE),#recall iso cdf\n",
    "                    mean(raw_cdf_perf[[6+(7*(i-1))]],na.rm=TRUE),sd(raw_cdf_perf[[6+(7*(i-1))]],na.rm=TRUE),#fpr iso cdf\n",
    "                    mean(raw_cdf_perf[[7+(7*(i-1))]],na.rm=TRUE))#alpha used in estimation\n",
    "\n",
    "    temp_ovr_perf=rbind(temp_ovr_perf,temp_ovr_perf1)\n",
    "    \n",
    "  }\n",
    "  return(temp_ovr_perf)\n",
    "}\n",
    "\n",
    "# ## For confidence intervals based on 100 runs\n",
    "error_tci=function(mean1,sd1,n2){\n",
    "    error1=qt(0.975,df=n2-1)*sd1/sqrt(n2)\n",
    "    return(error1)\n",
    "}\n",
    "\n",
    "## Overall Performance with Confidence intervals\n",
    "overall_performance_ci <- function(overall_performance2,n1){\n",
    "# n1=nrow(comb_perf)\n",
    "    overall_performance3=data.frame()\n",
    "    for (i in 1:nrow(overall_performance2)){\n",
    "        error1a=error_tci(overall_performance2[i,1],overall_performance2[i,2],n1)#recall CI\n",
    "        error1b=error_tci(overall_performance2[i,3],overall_performance2[i,4],n1)#1-ppv/FPR CI _rawcdf\n",
    "\n",
    "        error1c=error_tci(overall_performance2[i,5],overall_performance2[i,6],n1)#recall CI\n",
    "        error1d=error_tci(overall_performance2[i,7],overall_performance2[i,8],n1)#1-ppv/FPR CI _isocdf\n",
    "        \n",
    "\n",
    "        temp_perf3=c(\n",
    "                         overall_performance2[i,1],error1a,overall_performance2[i,3],error1b,#recall,fpr rawcdf\n",
    "                         overall_performance2[i,5],error1c,overall_performance2[i,7],error1d,#recall,fpr iso_cdf\n",
    "                         overall_performance2[i,10],overall_performance2[i,11],overall_performance2[i,12],overall_performance2[i,9])\n",
    "        overall_performance3=rbind(overall_performance3,temp_perf3)\n",
    "    }\n",
    "    return(overall_performance3)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Out of Bag estimation using Isolation Forest to estimate CDF of nominal data & train Iforest \n",
    "###### Here we are using 20% of the points for growing each tree, and grow 1000 trees\n",
    "###### use randomly selected 20% of the points to grow each tree, \n",
    "###### for each point, use the average depth from the trees which don't use this point to calculate anomaly score\n",
    "###### the following part is for getting anomaly scores for all clean data set, mixture data set and test data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If experiments needs to be done on LODA too uncomment all LODA related code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"run\"\n",
      "[1] 1\n",
      "[1] \"OCR\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in file(file, \"rt\"):\n",
      "“cannot open file './depth1_20_285_1.csv': No such file or directory”"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in file(file, \"rt\"): cannot open the connection\n",
     "output_type": "error",
     "traceback": [
      "Error in file(file, \"rt\"): cannot open the connection\nTraceback:\n",
      "1. read.csv(paste(\"./depth1_\", vpro, \"_\", n, \"_\", k, \".csv\", sep = \"\"), \n .     header = TRUE)",
      "2. read.table(file = file, header = header, sep = sep, quote = quote, \n .     dec = dec, fill = fill, comment.char = comment.char, ...)",
      "3. file(file, \"rt\")"
     ]
    }
   ],
   "source": [
    "for (run in 1:runs){\n",
    "  print(\"run\")\n",
    "  print(run) \n",
    "  ###########DATASETS\n",
    "#   source('~/data_sets_load.R')\n",
    "  source('~/R/workspace_r/jnrl papr exps/zz-trial_sep25/osu_iforest_trial/ICML-code/data_sets_trial.R')\n",
    "  ##############################make these as list of dataframes###########################\n",
    "table_freq=as.data.frame(table(data[[o_col]]))\n",
    "freq_known_t=list()\n",
    "data_known_t=list()\n",
    "data_unknown_t=list()\n",
    "no_known_class_inst=0\n",
    "for  (j in 1:length(known_classes)){\n",
    "   freq_known_t[[j]]=filter(table_freq,Var1==known_classes[j])[[2]][1];\n",
    "   no_known_class_inst=no_known_class_inst+freq_known_t[[j]];\n",
    "   data_known_t[[j]]=subset(data,data[[o_col]] %in% known_classes[j])\n",
    "   data_unknown_t[[j]]=subset(data,data[[o_col]] %in% setdiff(tr,known_classes[j]))\n",
    "   }\n",
    "\n",
    "#############each class proportion\n",
    "alpha_t=alpha\n",
    "prob_tr=list()\n",
    "alpha_class_t=list()\n",
    "for  (j in 1:length(known_classes)){\n",
    "  prob_tr[[j]]=freq_known_t[[j]]/no_known_class_inst;\n",
    "  alpha_class_t[[j]]=1-((1-alpha_t)*prob_tr[[j]]);\n",
    "}\n",
    "##\n",
    "  unknown_class=setdiff(tr,known_classes)\n",
    "  data_unknown=subset(data,data[[o_col]] %in% unknown_class)\n",
    "\n",
    "  data_known_b=list();\n",
    "  data_known_train=list();\n",
    "  data_known_valid=list();\n",
    "  data_known_valid_overall=data.frame();\n",
    "    \n",
    "  ### CLASSWISE data split for training seperate A.D for each class\n",
    "  \n",
    "    for (j in 1:length(known_classes)){\n",
    "    data_known_b[[j]]=subset(data,data[[o_col]] %in% known_classes[[j]])\n",
    "    # data_unknown_b[[j]]=subset(data,data[[o_col]] %in% setdiff(tr,known_classes[j]))\n",
    "    ####################  Now known_1 classes data split\n",
    "    n_known_inst=nrow(data_known_b[[j]])\n",
    "    ind_known_clas=1:nrow(data_known_b[[j]])\n",
    "    data_known_b[[j]]$ind=ind_known_clas\n",
    "    ind_known_t=sample(ind_known_clas,0.5*n_known_inst)\n",
    "    data_known_train[[j]]=data_known_b[[j]][ind_known_t,1:n_col]  \n",
    "    data_known_b[[j]]=data_known_b[[j]][-ind_known_t,1:n_col]\n",
    "\n",
    "    ind_known_clas=1:nrow(data_known_b[[j]])\n",
    "    data_known_b[[j]]$ind=ind_known_clas\n",
    "    ind_known_t=sample(ind_known_clas,0.5*n_known_inst)\n",
    "    data_known_valid[[j]]=data_known_b[[j]][ind_known_t,1:n_col]  \n",
    "    data_known_b[[j]]=data_known_b[[j]][-ind_known_t,1:n_col]\n",
    "    \n",
    "    # names(data_known_valid_overall)=names(data_known_valid[[j]])\n",
    "    data_known_valid_overall=rbind(data_known_valid_overall,data_known_valid[[j]])\n",
    "   \n",
    "  }\n",
    "  ##########################\n",
    "\n",
    "  n_known1=nrow(data_known_valid_overall)\n",
    "  data_known=data_known_valid_overall\n",
    "  n_unknown1=nrow(data_unknown)\n",
    "  n_known_tobe=as.integer(((1-alpha)/alpha)*n_unknown1)\n",
    "\n",
    "  if (n_known_tobe<n_known1){\n",
    "    data_known=data_known[sample(1:n_known1,n_known_tobe,replace = FALSE),]\n",
    "  }\n",
    "  N_row1=nrow(data_known)\n",
    "  N_anom=as.integer((alpha/(1-alpha))*(N_row1))\n",
    " ###############################\n",
    "  ind_unknown_clas=1:nrow(data_unknown)\n",
    "  data_unknown$ind=ind_unknown_clas\n",
    "  ind_unknown=sample(ind_unknown_clas,min(N_anom,nrow(data_unknown)))  \n",
    "  data_unknown_a=data_unknown[ind_unknown,1:n_col]\n",
    "\n",
    "  if (N_anom>nrow(data_unknown)){\n",
    "    print(\"not possible alpha in current setting\")\n",
    "  }\n",
    "\n",
    "  n_unknown_inst=nrow(data_unknown_a)\n",
    "  ind_unknown_clas=1:nrow(data_unknown_a)\n",
    "  data_unknown_a$ind=ind_unknown_clas\n",
    "  ind_test=sample(ind_unknown_clas,0.5*n_unknown_inst)  \n",
    "  \n",
    "  data_unknown_valid=data_unknown_a[ind_test,1:n_col]\n",
    "  data_unknown_test=data_unknown_a[-ind_test,1:n_col]\n",
    "  data_mix=rbind(data_unknown_valid,data_unknown_test,data_known_valid_overall)\n",
    "  #######################################\n",
    "  datab=c();loda_datab=c();\n",
    "    \n",
    "  n_test=nrow(data_mix)\n",
    "  n_classes=length(known_classes)\n",
    "  datan_t=data.frame(matrix(ncol = n_classes, nrow = n_test))\n",
    "  \n",
    "  \n",
    "  loda_datan_t=data.frame(matrix(ncol = n_classes, nrow = n_test))\n",
    "  loda_datan=c()\n",
    "    \n",
    "for  (j in 1:length(known_classes)){\n",
    "  n=nrow(data_known_train[[j]])#for iforest file naming.\n",
    "  k=j\n",
    "  \n",
    "  write.csv(data_known_train[[j]], file = paste(\"data1_\",vpro,\"_\",n,\"_\", k, \".csv\", sep = \"\"), row.names = FALSE)\n",
    "\n",
    "\n",
    "system(paste('./iforest','-i', paste('./data1_',vpro,'_',n,'_',k,'.csv', sep = \"\"),'-o', paste('./depth1_',vpro,'_',n,'_',k,'.csv', sep = \"\"),'-s', round(0.2*n), '-t 1000 -g -k -b',paste('./forest1_',vpro,'_', k,'.bin',sep = \"\")), wait = TRUE)\n",
    "score1 <- read.csv(paste('./depth1_',vpro,'_',n,'_',k,'.csv', sep = \"\"), header = TRUE)\n",
    "datab_t <- as.numeric(unlist(score1))\n",
    "system(paste('rm','-f',paste('./depth1_',vpro,'_',n,'_',k,'.csv', sep = \"\")))\n",
    "  datab=c(datab,datab_t)\n",
    "#     ## LODA CODE\n",
    "#     bt_out = btloda(data_known_train[[j]],sparsity=NA, maxk=50, keep=NULL, exclude=NULL, debug=F,inf_replace = log(1e-09))\n",
    "#     loda_datab_t= bt_out$oob_nll#loda oob from clean\n",
    "#     loda_datab=c(loda_datab,loda_datab_t)\n",
    "    \n",
    "#     loda_datan_t[[j]]= get_neg_ll_all_hist(data_mix, bt_out$pvh$w, bt_out$pvh$hists, inf_replace = log(1e-09))\n",
    "\n",
    "    system(paste('rm','-f',file = paste(\"data1_\",vpro,\"_\",n,\"_\", k, \".csv\", sep = \"\"), row.names = FALSE))\n",
    "    }\n",
    "    \n",
    "#     loda_datan=apply(loda_datan_t,1,FUN=min)\n",
    "\n",
    "\n",
    "for  (j in 1:length(known_classes)){\n",
    "  n=nrow(data_known_train[[j]])#for iforest file naming.\n",
    "  k=j\n",
    "  \n",
    "  write.csv(data_mix, file = paste(\"data2_\",vpro,\"_\",n,\"_\", k, \".csv\", sep = \"\"), row.names = FALSE)\n",
    "\n",
    "# system(paste('./iforest','-i', paste('./data2_',vpro,'_',n,'_',k,'.csv', sep = \"\"),'-o', paste('./depth1_',vpro,'_',n,'_',k,'.csv', sep = \"\"),'-s', round(0.2*n), '-t 1000 -g -k -b',paste('./forest1_',vpro,'_', k,'.bin',sep = \"\")), wait = TRUE)\n",
    "system(paste('./iforest','-i', paste('./data2_',vpro,'_',n,'_',k,'.csv', sep = \"\") ,'-o',paste('./depth2_',vpro,'_',n,'_',k,'.csv', sep = \"\"),'-g -f', paste('./forest1_',vpro,'_', k,'.bin',sep = \"\")), wait = TRUE)\n",
    "score1 <- read.csv(paste('./depth2_',vpro,'_',n,'_',k,'.csv', sep = \"\"), header = TRUE)\n",
    "datan_t1 <- as.numeric(unlist(score1))\n",
    "system(paste('rm','-f',paste('./depth2_',vpro,'_',n,'_',k,'.csv', sep = \"\")))\n",
    "datan_t[[j]]=datan_t1\n",
    "\n",
    "system(paste('rm','-f',file = paste(\"data2_\",vpro,\"_\",n,\"_\", k, \".csv\", sep = \"\"), row.names = FALSE))\n",
    "}\n",
    "datan=apply(datan_t,1,FUN=min)\n",
    "\n",
    "#############\n",
    "\n",
    "\n",
    "    temp_perf1_e02=cv_result(datab, datan, alpha, 0.02)\n",
    "    temp_perf1_e05=cv_result(datab, datan, alpha, 0.05)\n",
    "    temp_perf1_e10=cv_result(datab, datan, alpha, 0.1)\n",
    "\n",
    "#     loda_temp_perf1_e02=cv_result(loda_datab, loda_datan, alpha, 0.02)\n",
    "#     loda_temp_perf1_e05=cv_result(loda_datab, loda_datan, alpha, 0.05)\n",
    "#     loda_temp_perf1_e10=cv_result(loda_datab, loda_datan, alpha, 0.1)\n",
    "\n",
    "    temp_perf2_e02=as.data.frame(t(c(colMeans(temp_perf1_e02[1:10,]),colMeans(temp_perf1_e02[11:20,]),colMeans(temp_perf1_e02[21:30,]),colMeans(temp_perf1_e02[31:40,]),colMeans(temp_perf1_e02[41:50,]),colMeans(temp_perf1_e02[51:60,]))))\n",
    "    temp_perf2_e05=as.data.frame(t(c(colMeans(temp_perf1_e05[1:10,]),colMeans(temp_perf1_e05[11:20,]),colMeans(temp_perf1_e05[21:30,]),colMeans(temp_perf1_e05[31:40,]),colMeans(temp_perf1_e05[41:50,]),colMeans(temp_perf1_e05[51:60,]))))\n",
    "    temp_perf2_e10=as.data.frame(t(c(colMeans(temp_perf1_e10[1:10,]),colMeans(temp_perf1_e10[11:20,]),colMeans(temp_perf1_e10[21:30,]),colMeans(temp_perf1_e10[31:40,]),colMeans(temp_perf1_e10[41:50,]),colMeans(temp_perf1_e10[51:60,]))))\n",
    "\n",
    "#     loda_temp_perf2_e02=as.data.frame(t(c(colMeans(loda_temp_perf1_e02[1:10,]),colMeans(loda_temp_perf1_e02[11:20,]),colMeans(loda_temp_perf1_e02[21:30,]),colMeans(loda_temp_perf1_e02[31:40,]),colMeans(loda_temp_perf1_e02[41:50,]),colMeans(loda_temp_perf1_e02[51:60,]))))\n",
    "#     loda_temp_perf2_e05=as.data.frame(t(c(colMeans(loda_temp_perf1_e05[1:10,]),colMeans(loda_temp_perf1_e05[11:20,]),colMeans(loda_temp_perf1_e05[21:30,]),colMeans(loda_temp_perf1_e05[31:40,]),colMeans(loda_temp_perf1_e05[41:50,]),colMeans(loda_temp_perf1_e05[51:60,]))))\n",
    "#     loda_temp_perf2_e10=as.data.frame(t(c(colMeans(loda_temp_perf1_e10[1:10,]),colMeans(loda_temp_perf1_e10[11:20,]),colMeans(loda_temp_perf1_e10[21:30,]),colMeans(loda_temp_perf1_e10[31:40,]),colMeans(loda_temp_perf1_e10[41:50,]),colMeans(loda_temp_perf1_e10[51:60,]))))\n",
    "\n",
    "    names(temp_perf2_e02)=names(comp_perf1_e02)\n",
    "    names(temp_perf2_e05)=names(comp_perf1_e05)\n",
    "    names(temp_perf2_e10)=names(comp_perf1_e10)\n",
    "\n",
    "    comp_perf1_e02=rbind(comp_perf1_e02,temp_perf2_e02)\n",
    "    comp_perf1_e05=rbind(comp_perf1_e05,temp_perf2_e05)\n",
    "    comp_perf1_e10=rbind(comp_perf1_e10,temp_perf2_e10)\n",
    "\n",
    "#     names(loda_temp_perf2_e02)=names(comp_perf1_loda_e02)\n",
    "#     names(loda_temp_perf2_e05)=names(comp_perf1_loda_e05)\n",
    "#     names(loda_temp_perf2_e10)=names(comp_perf1_loda_e10)\n",
    "\n",
    "#     comp_perf1_loda_e02=rbind(comp_perf1_loda_e02,loda_temp_perf2_e02)\n",
    "#     comp_perf1_loda_e05=rbind(comp_perf1_loda_e05,loda_temp_perf2_e05)\n",
    "#     comp_perf1_loda_e10=rbind(comp_perf1_loda_e10,loda_temp_perf2_e10)\n",
    "    \n",
    "}#end runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We get the result's summary for various values of quantiles ( epsilon / q[paper] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "overall_perf_e02=overall_perf(comp_perf1_e02,0.02)\n",
    "overall_perf_e05=overall_perf(comp_perf1_e05,0.05)\n",
    "overall_perf_e10=overall_perf(comp_perf1_e10,0.10)\n",
    "\n",
    "# overall_perf_loda_e02=overall_perf(comp_perf1_loda_e02,0.02)\n",
    "# overall_perf_loda_e05=overall_perf(comp_perf1_loda_e05,0.05)\n",
    "# overall_perf_loda_e10=overall_perf(comp_perf1_loda_e10,0.10)\n",
    "\n",
    "\n",
    "overall_performance=data.frame()\n",
    "\n",
    "overall_performance=overall_perf_e02\n",
    "\n",
    "names(overall_perf_e05)=names(overall_performance)\n",
    "names(overall_perf_e10)=names(overall_performance)\n",
    "\n",
    "# names(overall_perf_loda_e02)=names(overall_performance)\n",
    "# names(overall_perf_loda_e05)=names(overall_performance)\n",
    "# names(overall_perf_loda_e10)=names(overall_performance)\n",
    "\n",
    "overall_performance=rbind(overall_performance,overall_perf_e05)\n",
    "overall_performance=rbind(overall_performance,overall_perf_e10)\n",
    "\n",
    "# overall_performance=rbind(overall_performance,overall_perf_loda_e02)\n",
    "# overall_performance=rbind(overall_performance,overall_perf_loda_e05)\n",
    "# overall_performance=rbind(overall_performance,overall_perf_loda_e10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### the final summary results are in this dataframe where the first 6 rows correspond to target quantile=0.02, then next 6 rows correspond to target quantile=0.05 and next 6 rows correspond to target quantile=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1=nrow(comp_perf1_e02)\n",
    "overall_performance_ci1=overall_performance_ci(overall_performance,n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
